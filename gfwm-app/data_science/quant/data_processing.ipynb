{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "esg_tickers = pd.read_csv(\"../Refinitiv ESG Final Data for Analysis.csv\")[\"Symbol\"]\n",
    "\n",
    "company_data = pd.read_csv(\"../company_data.csv\")\n",
    "\n",
    "# tickers for which we have financial data\n",
    "tickers = company_data[company_data['days_since_ipo'] > 180]['ticker']\n",
    "\n",
    "# use mask to filter out tickers found in esg dataset that do not exist and dual classes of stock\n",
    "# PEAK,\n",
    "# PXD (pioneer energy, aquired by exxon mobil),\n",
    "# WRK (sidney australia listing)\n",
    "# CDAY renamed to DAY\n",
    "# FLT listed in australia\n",
    "# GOOG (class C to GOOGL)\n",
    "# FOX (class B to FOXA)\n",
    "# NWS (class B to NWSA)\n",
    "\n",
    "esg_tickers = esg_tickers[~esg_tickers.isin([\"PEAK\", \"PXD\", \"WRK\", \"CDAY\", \"FLT\", \"GOOG\", \"FOX\", \"NWS\"])]\n",
    "\n",
    "# intersection of tickers in ESG data and the SP500 data we have\n",
    "# todo: only get the timeseries data for the ESG tickers we have\n",
    "tickers = pd.Series(list(set(esg_tickers) & set(tickers)), name = 'ticker').sort_values(ignore_index=True)\n",
    "\n",
    "# save for other files to use\n",
    "#tickers.to_csv(\"../tickers_to_keep.csv\", index=False)\n",
    "timeseries_13_18 = pd.read_csv(\"SP500_raw_timeseries_1-1-13--12-31-18.csv\")\n",
    "timeseries_19_24 = pd.read_csv(\"SP500_raw_timeseries_1-1-19--11-1-24.csv\")\n",
    "\n",
    "# remove columns (days) with no data, for example holidays\n",
    "timeseries_13_18 = timeseries_13_18.dropna(axis=1, how='all')\n",
    "timeseries_19_24 = timeseries_19_24.dropna(axis=1, how='all')\n",
    "\n",
    "# transpose so each column turns in a timeseries for one ticker\n",
    "# also reverse order so dates increase from top to bottom\n",
    "timeseries_13_18 = timeseries_13_18.set_index('ticker').T.iloc[::-1]\n",
    "timeseries_13_18.columns.name = None\n",
    "timeseries_19_24 = timeseries_19_24.set_index('ticker').T.iloc[::-1]\n",
    "timeseries_19_24.columns.name = None\n",
    "\n",
    "# Concatenate along the rows (axis=0)\n",
    "timeseries = pd.concat([timeseries_13_18.reset_index(), timeseries_19_24.reset_index()],\n",
    "                       ignore_index=True, sort=False)\n",
    "\n",
    "timeseries.rename(columns={timeseries.columns[0]: 'date'}, inplace=True)\n",
    "timeseries.set_index('date', inplace=True)\n",
    "timeseries.index = pd.to_datetime(timeseries.index)\n",
    "\n",
    "# only keep tickers we're interested in\n",
    "timeseries = timeseries.filter(items=tickers)\n",
    "spy_timeseries = pd.read_csv(\"spy_raw_timeseries_13-24.csv\").dropna(axis = 1, how='all')\n",
    "\n",
    "# transpose and reverse order so dates increase from top to bottom\n",
    "spy_timeseries = spy_timeseries.T.iloc[:0:-1]\n",
    "\n",
    "spy_timeseries.rename(columns={spy_timeseries.columns[0]: 'SPY'}, inplace=True)\n",
    "\n",
    "#print(spy_timeseries)\n",
    "spy_timeseries = (spy_timeseries/spy_timeseries.shift(1)).iloc[1:].map(np.log)\n",
    "\n",
    "spy_mean_log_return = spy_timeseries.mean() * 252\n",
    "spy_annual_volatility = spy_timeseries.std() * np.sqrt(252)\n",
    "\n",
    "#spy_timeseries.to_csv('spy_timeseries_13-24.csv')\n",
    "\n",
    "#print(spy_mean_log_return, spy_annual_volatility)\n",
    "# replace each entry with the log return compared to the previous day\n",
    "# first row will turn into NaN so remove it\n",
    "timeseries = (timeseries/timeseries.shift(1)).iloc[1:].map(np.log)\n",
    "#calculate performance averages for each ticker\n",
    "# mean log return is used in markowitz optimization\n",
    "# this volatility calculation is for information only\n",
    "\n",
    "#remember that each ticker is a column with dates increasing from top to bottom\n",
    "performance_summaries = pd.DataFrame(timeseries.mean(axis = 0) * 252, columns = ['mean_log_return'])\n",
    "\n",
    "performance_summaries['standard_deviation'] = timeseries.std(axis = 0) * np.sqrt(252)\n",
    "\n",
    "performance_summaries.rename_axis('ticker')\n",
    "\n",
    "#remember to write to csv as transpose so that tickers are easily filtered columns\n",
    "print(performance_summaries.T.head())\n",
    "# calculate covariance of all tickers\n",
    "# use cov_matrix to calculate correlation between all tickers\n",
    "# remove tickers that are too highly correlated\n",
    "\n",
    "# for every ticker, figure out the first day present in the dataset\n",
    "# then cov(A, B) and corr(A,B) only use the data that is present for both A and B\n",
    "\n",
    "first_valid_dates = [timeseries[ticker].first_valid_index()\n",
    "                       for ticker in tickers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_returns(r):\n",
    "    \n",
    "    if r <= 0:\n",
    "        return r * 1.1\n",
    "    \n",
    "    return r * 0.9\n",
    "\n",
    "def covariances(x, y):\n",
    "    \n",
    "    n = len(x)\n",
    "\n",
    "    x_bar = np.mean(x)\n",
    "    y_bar = np.mean(y)    \n",
    "    \n",
    "    raw_covariance = np.sum((x-x_bar) * (y-y_bar)) / (n-1)\n",
    "    \n",
    "    \n",
    "    # weight losers more heavily than winners\n",
    "    x_prime = np.where(x <= 0, x * 1.1, x * 0.9)\n",
    "    y_prime = np.where(y <= 0, y * 1.1, y * 0.9)\n",
    "    \n",
    "    x_prime_bar = np.mean(x_prime)\n",
    "    y_prime_bar = np.mean(y_prime)\n",
    "    \n",
    "    weighted_covariance = np.sum((x_prime - x_prime_bar) * \n",
    "                                 (y_prime - y_prime_bar)) / (n-1)\n",
    "    #print(raw_covariance, weighted_covariance, weighted_covariance/raw_covariance)\n",
    "    return raw_covariance, weighted_covariance\n",
    "    \n",
    "\n",
    "# calculate covariance of all tickers\n",
    "raw_cov_matrix = pd.DataFrame(np.nan, index=tickers, columns=tickers)\n",
    "adjusted_cov_matrix = pd.DataFrame(np.nan, index=tickers, columns=tickers)\n",
    "\n",
    "for i in range(len(tickers)):\n",
    "    #print(f\"calculating covariances for {tickers[i]}         \", end=\"\\r\", flush=True)\n",
    "    for j in range(i, len(tickers)):  # Loop over upper triangle\n",
    "\n",
    "        # start at the later date after which both tickers have data\n",
    "        start_date = pd.to_datetime(max(first_valid_dates[i], first_valid_dates[j]))\n",
    "        \n",
    "        ticker_i_slice = timeseries.loc[timeseries.index >= start_date, tickers[i]]\n",
    "        ticker_j_slice = timeseries.loc[timeseries.index >= start_date, tickers[j]]\n",
    "        \n",
    "        # calculate and write both raw and adjusted (weighted) covariances\n",
    "        raw, adj = covariances(ticker_i_slice, ticker_j_slice)\n",
    "        \n",
    "        raw_cov_matrix.iloc[i, j] = raw_cov_matrix.iloc[j, i] = raw\n",
    "        adjusted_cov_matrix.iloc[i, j] = adjusted_cov_matrix.iloc[j, i] = adj\n",
    "\n",
    "# use cov_matrix to calculate corr_matrix\n",
    "\n",
    "std_devs = np.sqrt(np.diagonal(adjusted_cov_matrix.values))\n",
    "# Create a matrix of standard deviations (outer product of std_devs with itself)\n",
    "# Compute correlation matrix\n",
    "corr_matrix = adjusted_cov_matrix / np.outer(std_devs, std_devs)\n",
    "'''\n",
    "# Apply upper triangle mask to get the upper triangle values\n",
    "# k=1 to exclude the diagonal\n",
    "upper_triangle = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1) == 1)).stack()\n",
    "\n",
    "high_correlations = [(index[0], index[1], value) \n",
    "                     for index, value in upper_triangle.items() \n",
    "                     if value > 0.95]\n",
    "\n",
    "print(high_correlations)\n",
    "\n",
    "# the highly correlated tickers are mostly different classes of the same ticker\n",
    "# [('FRT', 'REG', 0.9118897518213701), ('MET', 'PRU', 0.9005313958213659)]\n",
    "# significantly different enough over 10 years to leave both in.\n",
    "'''\n",
    "nan_count = adjusted_cov_matrix.isna().sum().sum()\n",
    "\n",
    "print(nan_count)\n",
    "\n",
    "#print(adjusted_cov_matrix.head())\n",
    "#print(raw_cov_matrix.head())\n",
    "n = len(tickers)\n",
    "\n",
    "# diagonal matrix of volatilities\n",
    "vol_matrix = np.diag(np.diag(adjusted_cov_matrix.values))\n",
    "\n",
    "# matrix of average correlations, except for diagonal of 1s\n",
    "rho = np.mean(corr_matrix)\n",
    "average_corr_matrix = np.full((n, n), rho)\n",
    "np.fill_diagonal(average_corr_matrix, 1)\n",
    "\n",
    "# F = V C V\n",
    "structured_cov_matrix = vol_matrix @ average_corr_matrix @ vol_matrix\n",
    "\n",
    "#print(f\"vol_matrix positive semi definite: {np.all(np.linalg.eigvals(vol_matrix) > 0)}\")\n",
    "#print(f\"average_corr_matrix positive semi definite: {np.all(np.linalg.eigvals(average_corr_matrix) > 0)}\")\n",
    "#print(f\"structured_cov_matrix positive semi definite: {np.all(np.linalg.eigvals(structured_cov_matrix) > 0)}\")\n",
    "\n",
    "\n",
    "print(\"Condition number of sample covariance matrix:\", np.linalg.cond(raw_cov_matrix))\n",
    "print(\"Condition number of adjusted covariance matrix:\", np.linalg.cond(adjusted_cov_matrix))\n",
    "\n",
    "print(f\"sample cov matrix positive semi definite: {np.all(np.linalg.eigvals(raw_cov_matrix) > 0)}\")\n",
    "print(f\"adjusted cov matrix positive semi definite: {np.all(np.linalg.eigvals(adjusted_cov_matrix) > 0)}\")\n",
    "\n",
    "delta = 0.5\n",
    "\n",
    "lw_adjusted_cov_matrix = (1-delta) * adjusted_cov_matrix + delta * structured_cov_matrix\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(lw_adjusted_cov_matrix)\n",
    "\n",
    "# Clip eigenvalues smaller than epsilon\n",
    "eigenvalues = np.clip(eigenvalues, 1e-6, None)\n",
    "lw_adjusted_cov_matrix = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n",
    "\n",
    "print(f\"determinant {np.linalg.det(lw_adjusted_cov_matrix)}\")\n",
    "\n",
    "print(\"lw adjusted cov matrix positive semidefinite?\", np.all(eigenvalues >= 0))\n",
    "\n",
    "print(\"Condition number of lw adjusted cov matrix:\", np.linalg.cond(lw_adjusted_cov_matrix))\n",
    "#write dataframes to csv\n",
    "\n",
    "adjusted_cov_matrix_df = pd.DataFrame(lw_adjusted_cov_matrix, columns=tickers)\n",
    "adjusted_cov_matrix_df.set_index(tickers, inplace=True)\n",
    "adjusted_cov_matrix_df.rename_axis('ticker', inplace=True)\n",
    "\n",
    "adjusted_cov_matrix_df.to_csv(\"sp500_adjusted_cov_matrix.csv\", index=True)\n",
    "raw_cov_matrix.set_index(tickers, inplace=True)\n",
    "raw_cov_matrix.rename_axis('ticker', inplace=True)\n",
    "\n",
    "print(raw_cov_matrix.head())\n",
    "raw_cov_matrix.to_csv(\"sp500_raw_cov_matrix.csv\", index = True)\n",
    "# print(timeseries.shape)\n",
    "timeseries.to_csv(\"sp500_timeseries_13-24.csv\", index=True)\n",
    "\n",
    "performance_summaries.T.to_csv(\"sp500_performance_summaries.csv\", index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
