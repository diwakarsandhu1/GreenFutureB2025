{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "esg_tickers = pd.read_csv(\"../Refinitiv ESG Final Data for Analysis.csv\")[\"Symbol\"]\n",
    "\n",
    "company_data = pd.read_csv(\"../company_data.csv\")\n",
    "\n",
    "# tickers for which we have financial data\n",
    "tickers = company_data[company_data['days_since_ipo'] > 180]['ticker']\n",
    "\n",
    "# use mask to filter out tickers found in esg dataset that do not exist and dual classes of stock\n",
    "# PEAK,\n",
    "# PXD (pioneer energy, aquired by exxon mobil),\n",
    "# WRK (sidney australia listing)\n",
    "# CDAY renamed to DAY\n",
    "# FLT listed in australia\n",
    "# GOOG (class C to GOOGL)\n",
    "# FOX (class B to FOXA)\n",
    "# NWS (class B to NWSA)\n",
    "\n",
    "esg_tickers = esg_tickers[~esg_tickers.isin([\"PEAK\", \"PXD\", \"WRK\", \"CDAY\", \"FLT\", \"GOOG\", \"FOX\", \"NWS\"])]\n",
    "\n",
    "# intersection of tickers in ESG data and the SP500 data we have\n",
    "# todo: only get the timeseries data for the ESG tickers we have\n",
    "tickers = pd.Series(list(set(esg_tickers) & set(tickers)), name = 'ticker').sort_values(ignore_index=True)\n",
    "\n",
    "# save for other files to use\n",
    "#tickers.to_csv(\"../tickers_to_keep.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_13_18 = pd.read_csv(\"SP500_raw_timeseries_1-1-13--12-31-18.csv\")\n",
    "timeseries_19_24 = pd.read_csv(\"SP500_raw_timeseries_1-1-19--11-1-24.csv\")\n",
    "\n",
    "# remove columns (days) with no data, for example holidays\n",
    "timeseries_13_18 = timeseries_13_18.dropna(axis=1, how='all')\n",
    "timeseries_19_24 = timeseries_19_24.dropna(axis=1, how='all')\n",
    "\n",
    "# transpose so each column turns in a timeseries for one ticker\n",
    "# also reverse order so dates increase from top to bottom\n",
    "timeseries_13_18 = timeseries_13_18.set_index('ticker').T.iloc[::-1]\n",
    "timeseries_13_18.columns.name = None\n",
    "timeseries_19_24 = timeseries_19_24.set_index('ticker').T.iloc[::-1]\n",
    "timeseries_19_24.columns.name = None\n",
    "\n",
    "# Concatenate along the rows (axis=0)\n",
    "timeseries = pd.concat([timeseries_13_18.reset_index(), timeseries_19_24.reset_index()],\n",
    "                       ignore_index=True, sort=False)\n",
    "\n",
    "timeseries.rename(columns={timeseries.columns[0]: 'date'}, inplace=True)\n",
    "timeseries.set_index('date', inplace=True)\n",
    "timeseries.index = pd.to_datetime(timeseries.index)\n",
    "\n",
    "# only keep tickers we're interested in\n",
    "timeseries = timeseries.filter(items=tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_timeseries = pd.read_csv(\"spy_raw_timeseries_13-24.csv\").dropna(axis = 1, how='all')\n",
    "\n",
    "# transpose and reverse order so dates increase from top to bottom\n",
    "spy_timeseries = spy_timeseries.T.iloc[:0:-1]\n",
    "\n",
    "spy_timeseries.rename(columns={spy_timeseries.columns[0]: 'SPY'}, inplace=True)\n",
    "\n",
    "#print(spy_timeseries)\n",
    "spy_timeseries = (spy_timeseries/spy_timeseries.shift(1)).iloc[1:].map(np.log)\n",
    "\n",
    "spy_mean_log_return = spy_timeseries.mean() * 252\n",
    "spy_annual_volatility = spy_timeseries.std() * np.sqrt(252)\n",
    "\n",
    "#spy_timeseries.to_csv('spy_timeseries_13-24.csv')\n",
    "\n",
    "#print(spy_mean_log_return, spy_annual_volatility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace each entry with the log return compared to the previous day\n",
    "# first row will turn into NaN so remove it\n",
    "timeseries = (timeseries/timeseries.shift(1)).iloc[1:].map(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           A      AAPL      ABBV      ABNB       ABT  \\\n",
      "mean_log_return     0.128544  0.205628  0.148640 -0.015110  0.110685   \n",
      "standard_deviation  0.259354  0.281245  0.264293  0.493811  0.228303   \n",
      "\n",
      "                        ACGL       ACN      ADBE       ADI       ADM  ...  \\\n",
      "mean_log_return     0.158220  0.136236  0.214281  0.138227  0.054555  ...   \n",
      "standard_deviation  0.249982  0.238814  0.318616  0.297966  0.261646  ...   \n",
      "\n",
      "                         WTW        WY      WYNN       XEL       XOM  \\\n",
      "mean_log_return     0.100943  0.005695 -0.017787  0.075307  0.021920   \n",
      "standard_deviation  0.224483  0.303562  0.465526  0.205717  0.262946   \n",
      "\n",
      "                         XYL       YUM       ZBH      ZBRA       ZTS  \n",
      "mean_log_return     0.124249  0.085279  0.041511  0.189462  0.150742  \n",
      "standard_deviation  0.264647  0.235212  0.259949  0.378174  0.254815  \n",
      "\n",
      "[2 rows x 478 columns]\n"
     ]
    }
   ],
   "source": [
    "#calculate performance averages for each ticker\n",
    "# mean log return is used in markowitz optimization\n",
    "# this volatility calculation is for information only\n",
    "\n",
    "#remember that each ticker is a column with dates increasing from top to bottom\n",
    "performance_summaries = pd.DataFrame(timeseries.mean(axis = 0) * 252, columns = ['mean_log_return'])\n",
    "\n",
    "performance_summaries['standard_deviation'] = timeseries.std(axis = 0) * np.sqrt(252)\n",
    "\n",
    "performance_summaries.rename_axis('ticker')\n",
    "\n",
    "#remember to write to csv as transpose so that tickers are easily filtered columns\n",
    "print(performance_summaries.T.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate covariance of all tickers\n",
    "# use cov_matrix to calculate correlation between all tickers\n",
    "# remove tickers that are too highly correlated\n",
    "\n",
    "# for every ticker, figure out the first day present in the dataset\n",
    "# then cov(A, B) and corr(A,B) only use the data that is present for both A and B\n",
    "\n",
    "first_valid_dates = [timeseries[ticker].first_valid_index()\n",
    "                       for ticker in tickers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_returns(r):\n",
    "    \n",
    "    if r <= 0:\n",
    "        return r * 1.1\n",
    "    \n",
    "    return r * 0.9\n",
    "\n",
    "def covariances(x, y):\n",
    "    \n",
    "    n = len(x)\n",
    "\n",
    "    x_bar = np.mean(x)\n",
    "    y_bar = np.mean(y)    \n",
    "    \n",
    "    raw_covariance = np.sum((x-x_bar) * (y-y_bar)) / (n-1)\n",
    "    \n",
    "    \n",
    "    # weight losers more heavily than winners\n",
    "    x_prime = np.where(x <= 0, x * 1.1, x * 0.9)\n",
    "    y_prime = np.where(y <= 0, y * 1.1, y * 0.9)\n",
    "    \n",
    "    x_prime_bar = np.mean(x_prime)\n",
    "    y_prime_bar = np.mean(y_prime)\n",
    "    \n",
    "    weighted_covariance = np.sum((x_prime - x_prime_bar) * \n",
    "                                 (y_prime - y_prime_bar)) / (n-1)\n",
    "    #print(raw_covariance, weighted_covariance, weighted_covariance/raw_covariance)\n",
    "    return raw_covariance, weighted_covariance\n",
    "    \n",
    "\n",
    "# calculate covariance of all tickers\n",
    "raw_cov_matrix = pd.DataFrame(np.nan, index=tickers, columns=tickers)\n",
    "adjusted_cov_matrix = pd.DataFrame(np.nan, index=tickers, columns=tickers)\n",
    "\n",
    "for i in range(len(tickers)):\n",
    "    #print(f\"calculating covariances for {tickers[i]}         \", end=\"\\r\", flush=True)\n",
    "    for j in range(i, len(tickers)):  # Loop over upper triangle\n",
    "\n",
    "        # start at the later date after which both tickers have data\n",
    "        start_date = pd.to_datetime(max(first_valid_dates[i], first_valid_dates[j]))\n",
    "        \n",
    "        ticker_i_slice = timeseries.loc[timeseries.index >= start_date, tickers[i]]\n",
    "        ticker_j_slice = timeseries.loc[timeseries.index >= start_date, tickers[j]]\n",
    "        \n",
    "        # calculate and write both raw and adjusted (weighted) covariances\n",
    "        raw, adj = covariances(ticker_i_slice, ticker_j_slice)\n",
    "        \n",
    "        raw_cov_matrix.iloc[i, j] = raw_cov_matrix.iloc[j, i] = raw\n",
    "        adjusted_cov_matrix.iloc[i, j] = adjusted_cov_matrix.iloc[j, i] = adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Apply upper triangle mask to get the upper triangle values\\n# k=1 to exclude the diagonal\\nupper_triangle = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1) == 1)).stack()\\n\\nhigh_correlations = [(index[0], index[1], value) \\n                     for index, value in upper_triangle.items() \\n                     if value > 0.95]\\n\\nprint(high_correlations)\\n\\n# the highly correlated tickers are mostly different classes of the same ticker\\n# [('FRT', 'REG', 0.9118897518213701), ('MET', 'PRU', 0.9005313958213659)]\\n# significantly different enough over 10 years to leave both in.\\n\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use cov_matrix to calculate corr_matrix\n",
    "\n",
    "std_devs = np.sqrt(np.diagonal(adjusted_cov_matrix.values))\n",
    "# Create a matrix of standard deviations (outer product of std_devs with itself)\n",
    "# Compute correlation matrix\n",
    "corr_matrix = adjusted_cov_matrix / np.outer(std_devs, std_devs)\n",
    "'''\n",
    "# Apply upper triangle mask to get the upper triangle values\n",
    "# k=1 to exclude the diagonal\n",
    "upper_triangle = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1) == 1)).stack()\n",
    "\n",
    "high_correlations = [(index[0], index[1], value) \n",
    "                     for index, value in upper_triangle.items() \n",
    "                     if value > 0.95]\n",
    "\n",
    "print(high_correlations)\n",
    "\n",
    "# the highly correlated tickers are mostly different classes of the same ticker\n",
    "# [('FRT', 'REG', 0.9118897518213701), ('MET', 'PRU', 0.9005313958213659)]\n",
    "# significantly different enough over 10 years to leave both in.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "nan_count = adjusted_cov_matrix.isna().sum().sum()\n",
    "\n",
    "print(nan_count)\n",
    "\n",
    "#print(adjusted_cov_matrix.head())\n",
    "#print(raw_cov_matrix.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number of sample covariance matrix: 85650.87924068997\n",
      "Condition number of adjusted covariance matrix: 67175.32044747974\n",
      "sample cov matrix positive semi definite: False\n",
      "adjusted cov matrix positive semi definite: False\n",
      "determinant 0.0\n",
      "lw adjusted cov matrix positive semidefinite? True\n",
      "Condition number of lw adjusted cov matrix: 33792.0771114569\n"
     ]
    }
   ],
   "source": [
    "n = len(tickers)\n",
    "\n",
    "# diagonal matrix of volatilities\n",
    "vol_matrix = np.diag(np.diag(adjusted_cov_matrix.values))\n",
    "\n",
    "# matrix of average correlations, except for diagonal of 1s\n",
    "rho = np.mean(corr_matrix)\n",
    "average_corr_matrix = np.full((n, n), rho)\n",
    "np.fill_diagonal(average_corr_matrix, 1)\n",
    "\n",
    "# F = V C V\n",
    "structured_cov_matrix = vol_matrix @ average_corr_matrix @ vol_matrix\n",
    "\n",
    "#print(f\"vol_matrix positive semi definite: {np.all(np.linalg.eigvals(vol_matrix) > 0)}\")\n",
    "#print(f\"average_corr_matrix positive semi definite: {np.all(np.linalg.eigvals(average_corr_matrix) > 0)}\")\n",
    "#print(f\"structured_cov_matrix positive semi definite: {np.all(np.linalg.eigvals(structured_cov_matrix) > 0)}\")\n",
    "\n",
    "\n",
    "print(\"Condition number of sample covariance matrix:\", np.linalg.cond(raw_cov_matrix))\n",
    "print(\"Condition number of adjusted covariance matrix:\", np.linalg.cond(adjusted_cov_matrix))\n",
    "\n",
    "print(f\"sample cov matrix positive semi definite: {np.all(np.linalg.eigvals(raw_cov_matrix) > 0)}\")\n",
    "print(f\"adjusted cov matrix positive semi definite: {np.all(np.linalg.eigvals(adjusted_cov_matrix) > 0)}\")\n",
    "\n",
    "delta = 0.5\n",
    "\n",
    "lw_adjusted_cov_matrix = (1-delta) * adjusted_cov_matrix + delta * structured_cov_matrix\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(lw_adjusted_cov_matrix)\n",
    "\n",
    "# Clip eigenvalues smaller than epsilon\n",
    "eigenvalues = np.clip(eigenvalues, 1e-6, None)\n",
    "lw_adjusted_cov_matrix = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n",
    "\n",
    "print(f\"determinant {np.linalg.det(lw_adjusted_cov_matrix)}\")\n",
    "\n",
    "print(\"lw adjusted cov matrix positive semidefinite?\", np.all(eigenvalues >= 0))\n",
    "\n",
    "print(\"Condition number of lw adjusted cov matrix:\", np.linalg.cond(lw_adjusted_cov_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write dataframes to csv\n",
    "\n",
    "adjusted_cov_matrix_df = pd.DataFrame(lw_adjusted_cov_matrix, columns=tickers)\n",
    "adjusted_cov_matrix_df.set_index(tickers, inplace=True)\n",
    "adjusted_cov_matrix_df.rename_axis('ticker', inplace=True)\n",
    "\n",
    "adjusted_cov_matrix_df.to_csv(\"sp500_adjusted_cov_matrix.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticker         A      AAPL      ABBV      ABNB       ABT      ACGL       ACN  \\\n",
      "ticker                                                                         \n",
      "A       0.000267  0.000127  0.000099  0.000176  0.000132  0.000095  0.000135   \n",
      "AAPL    0.000127  0.000314  0.000074  0.000218  0.000105  0.000088  0.000134   \n",
      "ABBV    0.000099  0.000074  0.000277  0.000004  0.000105  0.000073  0.000084   \n",
      "ABNB    0.000176  0.000218  0.000004  0.000968  0.000087  0.000070  0.000174   \n",
      "ABT     0.000132  0.000105  0.000105  0.000087  0.000207  0.000084  0.000112   \n",
      "\n",
      "ticker      ADBE       ADI       ADM  ...       WTW        WY      WYNN  \\\n",
      "ticker                                ...                                 \n",
      "A       0.000159  0.000163  0.000096  ...  0.000096  0.000146  0.000165   \n",
      "AAPL    0.000188  0.000178  0.000081  ...  0.000090  0.000140  0.000170   \n",
      "ABBV    0.000096  0.000090  0.000075  ...  0.000070  0.000096  0.000110   \n",
      "ABNB    0.000273  0.000270  0.000069  ...  0.000096  0.000168  0.000346   \n",
      "ABT     0.000128  0.000115  0.000079  ...  0.000086  0.000118  0.000101   \n",
      "\n",
      "ticker       XEL       XOM       XYL       YUM       ZBH      ZBRA       ZTS  \n",
      "ticker                                                                        \n",
      "A       0.000059  0.000089  0.000144  0.000097  0.000117  0.000182  0.000138  \n",
      "AAPL    0.000060  0.000085  0.000121  0.000093  0.000101  0.000183  0.000124  \n",
      "ABBV    0.000053  0.000080  0.000083  0.000066  0.000098  0.000096  0.000101  \n",
      "ABNB   -0.000014  0.000096  0.000175  0.000113  0.000139  0.000300  0.000154  \n",
      "ABT     0.000072  0.000065  0.000107  0.000083  0.000111  0.000128  0.000121  \n",
      "\n",
      "[5 rows x 478 columns]\n"
     ]
    }
   ],
   "source": [
    "raw_cov_matrix.set_index(tickers, inplace=True)\n",
    "raw_cov_matrix.rename_axis('ticker', inplace=True)\n",
    "\n",
    "print(raw_cov_matrix.head())\n",
    "raw_cov_matrix.to_csv(\"sp500_raw_cov_matrix.csv\", index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(timeseries.shape)\n",
    "timeseries.to_csv(\"sp500_timeseries_13-24.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "performance_summaries.T.to_csv(\"sp500_performance_summaries.csv\", index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
